# WHOP LLM Configuration
# All variables use the WHOP_ prefix

# Provider: "none", "ollama", "llamacpp", "vllm", "claude", "openai"
WHOP_LLM_PROVIDER=none

# Ollama (local, no API key needed)
WHOP_OLLAMA_MODEL=llama3
WHOP_OLLAMA_BASE_URL=http://localhost:11434

# llama.cpp server (local, no API key needed)
WHOP_LLAMACPP_SERVER_URL=http://localhost:8080

# vLLM server (local, serves HuggingFace models, no API key needed)
WHOP_VLLM_SERVER_URL=http://localhost:8000
WHOP_VLLM_MODEL=meta-llama/Llama-3-8b-chat-hf

# Anthropic Claude (cloud, requires API key)
WHOP_ANTHROPIC_API_KEY=
WHOP_ANTHROPIC_MODEL=claude-sonnet-4-20250514

# OpenAI (cloud, requires API key)
WHOP_OPENAI_API_KEY=
WHOP_OPENAI_MODEL=gpt-4o

# Shared LLM settings
WHOP_LLM_TEMPERATURE=0.3
WHOP_LLM_MAX_TOKENS=2048
WHOP_LLM_TIMEOUT_SECONDS=30
