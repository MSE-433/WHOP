# WHOP Configuration
# All variables use the WHOP_ prefix

# --- Database ---
# Docker: /data/whop.db (mapped to a volume, set automatically by docker-compose)
# Local: whop.db (default, relative to backend/)
# WHOP_DB_PATH=whop.db

# --- CORS ---
# Docker: http://localhost:3000 (the exposed frontend port)
# Local dev: http://localhost:5173 (Vite), http://localhost:3000
# WHOP_CORS_ORIGINS=["http://localhost:3000","http://localhost:5173"]

# --- LLM Provider ---
# Provider: "none", "ollama", "llamacpp", "vllm", "claude", "openai"
WHOP_LLM_PROVIDER=none

# Ollama (local, no API key needed)
# Docker: http://ollama:11434  |  Local: http://localhost:11434
WHOP_OLLAMA_MODEL=llama3
WHOP_OLLAMA_BASE_URL=http://localhost:11434

# llama.cpp server (local, no API key needed)
# Docker: http://llamacpp:8080  |  Local: http://localhost:8080
WHOP_LLAMACPP_SERVER_URL=http://localhost:8080
# GGUF model filename in backend/LLMs/ (Docker only)
LLAMACPP_MODEL_FILE=qwen2.5-7b-instruct-q4_k_m-00001-of-00002.gguf

# vLLM server (local, serves HuggingFace models, no API key needed)
WHOP_VLLM_SERVER_URL=http://localhost:8000
WHOP_VLLM_MODEL=meta-llama/Llama-3-8b-chat-hf

# Anthropic Claude (cloud, requires API key)
WHOP_ANTHROPIC_API_KEY=
WHOP_ANTHROPIC_MODEL=claude-sonnet-4-20250514

# OpenAI (cloud, requires API key)
WHOP_OPENAI_API_KEY=
WHOP_OPENAI_MODEL=gpt-4o

# Shared LLM settings
WHOP_LLM_TEMPERATURE=0.3
WHOP_LLM_MAX_TOKENS=2048
WHOP_LLM_TIMEOUT_SECONDS=30
